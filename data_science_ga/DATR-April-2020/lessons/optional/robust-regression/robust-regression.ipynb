{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Robust Regression\n",
    "\n",
    "_Authors: Greg Baker (SYD)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives \n",
    "- **Review** the limitations and shortcomings of OLS and the $R^2$ scoring function.\n",
    "- **Define** when to use robust regression methods instead of OLS.\n",
    "- **Demonstrate** how Theil-Sen, RANSAC, and Huber work and what they are optimizing for.\n",
    "- **Explain** the advantages of using median absolute error as a scoring function.\n",
    "- **Create** a scoring function that's appropriate for different business scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "- [Review: Ordinary Least Squares and Squared Error Loss](#ols)\n",
    "- [Robust Regression](#robust)\n",
    "   - [Theil-Sen](#theilsen)\n",
    "   - [RANSAC](#ransac)\n",
    "   - [Huber](#huber)\n",
    "- [Median Absolute Error](#mae)\n",
    "- [Scenario-Specific Scoring Functions](#custom)\n",
    "    - [Qiqi's Lemonade Stand](#qiqi-lemonade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"introduction\"></a>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Congratulations! You've mastered linear regression. This is one of the most important skills a data scientist can have.\n",
    "\n",
    "<img src=\"assets/kdnuggets-plot.jpg\">\n",
    "\n",
    "In this lesson, you'll learn some of the techniques data scientists use to leverage linear regression effectively in a commercial environment, including:\n",
    "\n",
    "- A review of ordinary least squares.\n",
    "- Robust regression.\n",
    "- Useful metrics for evaluating a line.\n",
    "\n",
    "This knowledge helps data scientists drive real value for an organization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ols\"></a>\n",
    "\n",
    "## Review: Ordinary Least Squares ($R^2$)\n",
    "\n",
    "---\n",
    "\n",
    "So far, most examples have used ordinary least squares (OLS) regression to find a good linear model. With this technique, we look at all possible linear models and select one.\n",
    "\n",
    "> When you look at what this model predicts and compare it to what was in the training data, there will be errors. For each point, you'll look at the square of the difference between what was predicted and what it was supposed to be, then sum up those squares. _The line you are looking for will be the one with the smallest sum._\n",
    "\n",
    "Occasionally, this is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# To illustrate some of these concepts, we'll make use of the following function to draw plots of numbers with lines through them to show the size of the error in each case.\n",
    "\n",
    "def fancy_linear_plot(actuals, model, ax, include_error_bars=True):\n",
    "    ax.scatter(x=actuals.index, y=actuals)\n",
    "    predictions = pd.Series(index=actuals.index, data=model.predict(actuals.index.values.reshape(-1,1)))\n",
    "    predictions.plot(c='red', ax=ax)\n",
    "    squared_difference = 0\n",
    "    differences = []\n",
    "    for x in predictions.index:\n",
    "        true_y = actuals.loc[x]\n",
    "        predicted_y = predictions.loc[x]\n",
    "        error_line = pd.Series(index=[x,x],data=[predicted_y,true_y])\n",
    "        difference = true_y - predicted_y\n",
    "        squared_difference += difference * difference\n",
    "        differences.append(\"%.1f\" % (difference * difference,))\n",
    "        if include_error_bars:\n",
    "            error_line.plot(ax=ax, linestyle='dashed', c='green')\n",
    "            ax.annotate(\"error=%d\" % (difference,),xy=(x+0.1,true_y-2 if true_y > predicted_y else true_y+2))\n",
    "    if include_error_bars:\n",
    "        title = \"Sum of squares difference =\\n%s\\n = %d\"  % (\" + \".join(differences), squared_difference)\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# With this example data, let's look at four potential linear models and what they look like.\n",
    "\n",
    "example_data = pd.Series(\n",
    "    data=[9,21,15,35,51,68,67,80],\n",
    "    index=[1,2,3,4,5,6,7,8]\n",
    ")\n",
    "\n",
    "\n",
    "(fig, axes) = plt.subplots(nrows=2, ncols=2, figsize=(18,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "lr1 = sklearn.linear_model.LinearRegression()\n",
    "lr1.coef_ = np.array([20])\n",
    "lr1.intercept_ = -40\n",
    "fancy_linear_plot(example_data, lr1, axes[0][0])\n",
    "\n",
    "lr2 = sklearn.linear_model.LinearRegression()\n",
    "lr2.coef_ = np.array([10])\n",
    "lr2.intercept_ = 0\n",
    "fancy_linear_plot(example_data, lr2, axes[0][1])\n",
    "\n",
    "\n",
    "lr3 = sklearn.linear_model.LinearRegression()\n",
    "lr3.coef_ = np.array([3])\n",
    "lr3.intercept_ = 30\n",
    "fancy_linear_plot(example_data, lr3, axes[1][0])\n",
    "\n",
    "lr4 = sklearn.linear_model.LinearRegression()\n",
    "lr4.coef_ = np.array([-5])\n",
    "lr4.intercept_ = 50\n",
    "fancy_linear_plot(example_data, lr4, axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# But, with ordinary least squares, we can get the one that has the smallest sum of squares.\n",
    "(fig, ax) = plt.subplots(figsize=(9,5))\n",
    "lr3 = sklearn.linear_model.LinearRegression()\n",
    "lr3.fit(example_data.index.values.reshape(-1,1), example_data)\n",
    "fancy_linear_plot(example_data, lr3, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test Your Knowledge\n",
    "\n",
    "1) True or false: The line generated by the ordinary least squares regression library in\n",
    "   scikit-learn is the line that minimizes $R^2$.\n",
    "   \n",
    "2) True or false: Ordinary least squares regression only works when your data have\n",
    "   two dimensions: one-dimensional $y$ and one-dimensional $X$.\n",
    "   \n",
    "3) True or false: The line found by minimizing $R^2$ isn't affected much by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why We Like $R^2$\n",
    "\n",
    "- It's a unique answer.\n",
    "- It's easy to calculate because we can use derivatives to create a formula to find it quickly.\n",
    "- It makes sure that big mistakes are as small as possible. Simply making a small adjustment to the line to shrink the square of a big error can be worthwhile, even if it means losing accuracy on some small errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Problems With $R^2$\n",
    "\n",
    "- **One outlier can ruin your whole model**. Ordinary least squares requires each outlier in one direction to have a corresponding quantity of outliers in the other direction (heteroskedasticity).\n",
    "\n",
    "- This assumes that your $X$ values are measured with no error.\n",
    "- **To a business, the dollar cost of a faulty prediction probably doesn't correspond** to the square of how wrong that prediction was. Typically, we want to **maximize the company's profits**, not minimize the squared error in our predictions.\n",
    "\n",
    "Therefore, outside of specific theoritical situations, it is very rare for $R^2$ to be the right technique to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "# What Should We Use *Instead* of Ordinary Least Squares?\n",
    "\n",
    "- **Regularization**: This is one option when you want to find a simple model.\n",
    "\n",
    "- **Robust regression**: This is a great option when your main challenge is finding a signal in a noisy data set.\n",
    "\n",
    "Both of these methods are usually used in conjunction with **custom scoring functions**, especially when you know the cost to the business of the errors in your model. We'll be covering robust regression here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"#robust\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "## Robust Regression\n",
    "\n",
    "Robust regression is a technique used to generate a linear model that's *the best* without being sensitive to outliers and noise. Some of its methods are so simple that they can be performed by hand, even on large data sets. Some, however, are so complex that they can only be done with modern computers.\n",
    "They differ in how they define *the best linear model* and their results will rarely be the same as the least squares linear model. Let's look at an example where the least squares line is obviously incorrect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First, draw the picture without any line through it.\n",
    "noisy_example = pd.Series(\n",
    "    data=[1,9,150,27,35,51,60,67,80],\n",
    "    index=[0,1,2,3,4,5,6,7,8]\n",
    ")\n",
    "(fig, ax) = plt.subplots(figsize=(14,5), ncols=2)\n",
    "ax[0].scatter(x=noisy_example.index, y=noisy_example)\n",
    "\n",
    "# Create another chart with an ordinary least squares model overlay.\n",
    "ols = sklearn.linear_model.LinearRegression()\n",
    "ols.fit(noisy_example.index.values.reshape(-1,1), noisy_example)\n",
    "fancy_linear_plot(noisy_example, ols, ax[1], include_error_bars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Class Exercise\n",
    "\n",
    "1) In plain English, describe why the ordinary least squares model isn't effective in modeling this data set.\n",
    "- Why doesn't minimizing the sum of the square of the error terms model this data well?\n",
    "2) What would be a better metric? How could you compare two lines and decide which one fits the data better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"theilsen\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "## Theil-Sen\n",
    "\n",
    "(Pronounced \"Tail-Sen.\")\n",
    "\n",
    "The Theil-Sen method is simple and yet works surprisingly well.\n",
    "\n",
    "1) Pick a pair of points at random.\n",
    "2) Draw a line through them and note the gradient of the line.\n",
    "3) Repeat Steps 1 and 2 some number of times (e.g. 25 times).\n",
    "4) Sort the lines by gradient.\n",
    "5) Choose the line with the median gradient.\n",
    "\n",
    "(If you have more than two columns in your $X$ DataFrame, choose three points and draw a plane. If you have three columns, choose four points and create a hyperplane, and so on.)\n",
    "\n",
    "**Theil-Sen's definition of best**: A linear model that isn't extreme; a \"typical\" linear model in which all of the other reasonable linear models are spread out evenly above and below the one that was chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "<img src=\"assets/theilsen.jpg\" style=\"width: 400px; float: left; margin: 24px\">\n",
    "\n",
    "### Optional Group Activity\n",
    "\n",
    "The best way to understand Theil-Sen is to do it by hand.\n",
    "\n",
    "1) Clear a big space in the classroom. <br>\n",
    "2) Create some numbered tokens. You can just tear up small pieces of paper and write distinct numbers on them. <br>\n",
    "3) Start in one corner of the room, scattering the tokens on the floor in a vaguely straight line. Throw some tokens around randomly to act as outliers.\n",
    "4) Use the handy random number generator in the next cell (or some dice, or your phones, etc) to choose a random pair of tokens. If you have some string and tape, you can stick a line on the floor that runs through those two points. If it's a surface that you can use chalk on, you could draw the line instead. If you only have stationery, you could place a pencil or pen on the floor in a line. <br>\n",
    "5) Keep generating random pairs of points. Seven pairs should be enough, but you could do nine or 11. Sometimes you'll get crazy lines that are obviously wrong. <br>\n",
    "6) Find the *middle* line out of the lines you drew. Did that produce a *reasonable* line through your data? Yes or no?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import random\n",
    "number_of_tokens = 50\n",
    "display(HTML(\n",
    "    '<h1 style=\"color: red; font-size: 192px; margin: 48px\">%d &amp; %d</h1>' % (\n",
    "        random.randint(1,number_of_tokens),random.randint(1,number_of_tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ransac\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "## RANSAC\n",
    "\n",
    "The RANSAC method is nearly as simple as Theil-Sen and can handle data sets that have a large number of outliers and a lot of random noise. Even if nearly 50 percent of your data set is noise, RANSAC can still find hidden trends that most other algorithms can't.\n",
    "\n",
    "1) Pick a value or distance that you think is *close enough.*\n",
    "2) Pick a random pair of points. Draw a line through them and then count the number of points that are *close enough* to the line.\n",
    "3) Pick another pair of points, draw a line, and count the number of points that are *close enough* to the new line. If this line was better than the existing line, discard the existing line; otherwise, discard the new line.\n",
    "4) Repeat Step 3 until you haven't seen an improvement in a long time. The line you have is your RANSAC best line. <br>\n",
    "\n",
    "Like RANSAC, this works best in higher dimensions: Use triples of points forming a plane if you have two columns in your DataFrame.\n",
    "\n",
    "RANSAC's definition of the **best linear model** is the one that's close to a lot of points; the one that models the data accurately *the most often*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "<img src=\"assets/ransac.jpg\" style=\"float: left; width: 300px; margin-right: 24px\">\n",
    "\n",
    "### Optional Group Activity\n",
    "\n",
    "This is similar to the Theil-Sen activity. If you still have markers on the floor, you can reuse them!\n",
    "\n",
    "1) Find a convenient measuring device. If you have a ruler, that's good. Otherwise, you can use a shoe or anything else you can run along a line. <br>\n",
    "\n",
    "2) Follow the RANSAC procedure: Pick a random pair of points and identify the line between them. One person carries the shoe/ruler/guide along the line; another person keeps count of the number of tokens it passes over. <br>\n",
    "\n",
    "3) For this exercise, you'll usually have found a good line after 10 pairs of points. Beyond that, it's rare to find a better line (one where the shoe/ruler passes over more tokens). <br>\n",
    "\n",
    "> In the picture on the left, the ruler passed over 18 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to Code!\n",
    "\n",
    "Let's see how these algorithms work on the data set we just explored where ordinary least squares performed so poorly.\n",
    "\n",
    "The scikit-learn library includes functions for Theil-Sen and RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(figsize=(14,5), ncols=2)\n",
    "\n",
    "ts = sklearn.linear_model.TheilSenRegressor()\n",
    "ts.fit(noisy_example.index.values.reshape(-1,1), noisy_example)\n",
    "fancy_linear_plot(noisy_example, ts, ax[0], include_error_bars=False)\n",
    "\n",
    "ransac = sklearn.linear_model.RANSACRegressor()\n",
    "ransac.fit(noisy_example.index.values.reshape(-1,1), noisy_example)\n",
    "fancy_linear_plot(noisy_example, ransac.estimator_, ax[1], include_error_bars=False)\n",
    "\n",
    "ax[0].annotate(\"Theil-Sen\", (5,10), size=18)\n",
    "ax[1].annotate(\"RANSAC\", (5,10), size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A Reminder\n",
    "\n",
    "Theil-Sen and RANSAC have their own definitions for *the best linear model*. If you measure them with $R^2$, they will do quite badly!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(figsize=(18,7), ncols=3)\n",
    "\n",
    "fancy_linear_plot(noisy_example, ts, ax[0])\n",
    "fancy_linear_plot(noisy_example, ransac, ax[1])\n",
    "fancy_linear_plot(noisy_example, ols, ax[2])\n",
    "ax[0].annotate(\"Theil-Sen\", (5,10), size=18)\n",
    "ax[1].annotate(\"RANSAC\", (5,10), size=18)\n",
    "ax[2].annotate(\"Least Squares\", (4,10), size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Huber\n",
    "\n",
    "The **Huber** regressor minimizes the sum of:\n",
    "- The absolute value of the prediction error for predictions that were wrong by a large margin.\n",
    "- The square of the value of the prediction error for predictions that were close.\n",
    "\n",
    "This turns out to be very effective and is one of the most-used robust linear regressors. Outliers don't have disproportionate effects, but Huber will still try to minimize local errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(figsize=(8,5), ncols=1)\n",
    "\n",
    "huber = sklearn.linear_model.HuberRegressor()\n",
    "huber.fit(noisy_example.index.values.reshape(-1,1), noisy_example)\n",
    "fancy_linear_plot(noisy_example, huber, ax, include_error_bars=False)\n",
    "\n",
    "ax.annotate(\"Huber\", (6.5,10), size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Regressors\n",
    "\n",
    "| **Regressor**          | **Handles Outliers...**   | **Useful When...**  |\n",
    "|------------------------|------------|----------------------------------|\n",
    "| Huber                  | Tolerant of unbalanced outliers.        | Usually the best one to try first. |\n",
    "| Theil-Sen              | Unaffected if < 29 percent of data.     | Your $X$ values have errors and noise. |\n",
    "| RANSAC                 | Unaffected if < 50 percent of data.     | Your $Y$ values have errors and noise. |\n",
    "| Ordinary Least Squares | Only if normally distributed (rare).    | Avoiding large errors is important. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test Your Knowledge\n",
    "\n",
    "1) What's the difference between a robust regressor and a non-robust regressor?\n",
    "\n",
    "2) What are the three robust regressors you've learned about today and how do they differ? What are the pros/cons of each?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scoring\n",
    "\n",
    "How do we know whether or not a regressor has worked well? What do we mean by *best* line?\n",
    "\n",
    "There are several ways of scoring a regressor:\n",
    "\n",
    "- **$R^2$**: If you use this, then ordinary least squares will get you the best line.\n",
    "\n",
    "- **Median absolute error**: If this is your measure, you will cope with large numbers of outliers.\n",
    "\n",
    "- **Dollar value loss**: If this is your measure, your organization will thank you.\n",
    "\n",
    "\n",
    "<font size=-5>$^*$ YMMV</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Median Absolute Error\n",
    "\n",
    "Scikit-learn includes several different out-of-the-box scoring functions for regressors and more than a dozen scoring functions for classifiers.\n",
    "\n",
    "**Median absolute error** is commonly used for **noisy data**. You will have some points that you predicted well (so the absolute value of the error on those points will be quite small) and some that you predicted badly (so the absolute value of the error on those points will be quite large). What's the median of those points? Fifty percent of your errors fall below this number.\n",
    "\n",
    "This (and many other) metric functions are defined in `sklearn.metrics.`, but for many you can use a shorthand string (e.g., `'median_absolute_error'`) anywhere that a metric is passed as an argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# It doesn't normally make sense to measure how well a regressor predicts on data it was given for training, but here we'll do it anyway. \n",
    "\n",
    "import sklearn.metrics\n",
    "def score(model):\n",
    "    predictions = model.predict(noisy_example.index.values.reshape(-1,1))\n",
    "    return sklearn.metrics.median_absolute_error(noisy_example, predictions)\n",
    "\n",
    "print(\"OLS:\",score(ols))\n",
    "print(\"Theil-Sen:\", score(ts))\n",
    "print(\"RANSAC:\", score(ransac))\n",
    "print(\"Huber:\", score(huber))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I.e., for Huber, half of the points were predicted within 1.82 of the actual value. Or, to consider it negatively, for half of the points, ordinary least squares was more than 18.3333 away from the actual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name='custom'></a>\n",
    "\n",
    "----\n",
    "\n",
    "# Scenario-Specific Scoring Functions\n",
    "\n",
    "$R^2$ is reasonably common (and often misapplied). *Median absolute error* is commonly used, as well. But **the most common scoring method** measures the error in **dollars** (or whatever currency your business uses).\n",
    "\n",
    "In most organizations, your model will produce a prediction that's used to help with decision-making, and there will be costs associated an error. Often, those costs will be **asymmetric** (overestimating might be more expensive than underestimating).\n",
    "\n",
    "In some cases, you may work for a startup or organization that isn't very concerned with dollar costs. In this case, the right metric may involve monthly users, sign-ups, or other growth-relevant metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "<a name=\"qiqi-lemonade\"></a>\n",
    "\n",
    "## Lemonade Stand\n",
    "\n",
    "<img src=\"assets/lemonade-stand.jpg\" style=\"width: 200px; float: left; margin: 20px\">\n",
    "\n",
    "Let's say it costs Qiqi 3 cents to buy the sugar, lemons, and cups necessary to make one glass of lemonade. She has to spend this money to buy the ingredients upfront at the beginning of the day - and can't buy more until the next day if she runs out.\n",
    "\n",
    "She pays her brother a fixed fee of \\$2 to make the mixture, regardless of how many cups she sells each day. She charges customers 50 cents.\n",
    "\n",
    "Qiqi has access to data showing that there is a relationship between the temperature and humidity and the number of customers who will want to buy a cup of lemonade. She is trying to choose between several linear models.\n",
    "\n",
    "**Question**: What would be good scoring function to help Qiqi evaluate which model to use?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "First, discard a situation where Qiqi decides not to be in business at all! Next, the \\$2 to her brother is a fixed cost; therefore, we can ignore it.\n",
    "\n",
    "In the test portion of the database, let $X_p$ be the predicted number of customers and $X_a$ be the actual number of customers for a particular day.\n",
    "\n",
    "- If $X_p > X_a$, then Qiqi would lose $0.03 * (X_p - X_a)$ in wasted materials if she had followed that model.\n",
    "\n",
    "- If $X_a > X_p$, then Qiqi would lose $0.50 * (X_a - X_p)$ in lost sales if she had followed that model.\n",
    "\n",
    "- If $X_p = X_a$, then Qiqi would waste nothing and lose no sales.\n",
    "\n",
    "The closer the score is to zero, the better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lemonade_scorer(estimator, X, y):\n",
    "    predictions = list(estimator.predict(X))\n",
    "    y = list(y)\n",
    "    score = 0.0\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] > y[i]:\n",
    "            score -= 0.03 * int(predictions[i] - y[i])\n",
    "        elif predictions[i] < y[i]:\n",
    "            score -= 0.5 * int(y[i] - predictions[i])\n",
    "        else:\n",
    "            score -= 0.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lemonade_stand = pd.read_csv('datasets/lemonade-stand.csv', index_col=0)\n",
    "lemonade_stand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# There is a relationship between temperature and sales.\n",
    "# There is a (weaker) relationship between increased humidity and decreased sales, which could have\n",
    "# something to do with whether or not it was raining!\n",
    "# However, there are still about five days in which sales were much lower than would have been expected.\n",
    "sns.pairplot(lemonade_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets. If we train on the training set, we can then get an estimate of the dollar cost of the model by trying it out on the testing set.\n",
    "(Xtrain, Xtest, Ytrain, Ytest) = sklearn.model_selection.train_test_split(lemonade_stand[['Max_Temp_C','humidity']],\n",
    "                                                                         lemonade_stand.sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try out our four different linear models: OLS, Theil-Sen, RANSAC, and Huber.\n",
    "lemonade_ols = sklearn.linear_model.LinearRegression()\n",
    "lemonade_ts = sklearn.linear_model.TheilSenRegressor()\n",
    "lemonade_ransac = sklearn.linear_model.RANSACRegressor()\n",
    "lemonade_huber = sklearn.linear_model.HuberRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Call .fit() on each model using the training data.\n",
    "lemonade_ols.fit(Xtrain, Ytrain)\n",
    "lemonade_ts.fit(Xtrain, Ytrain)\n",
    "lemonade_ransac.fit(Xtrain, Ytrain)\n",
    "lemonade_huber.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use Matplotlib to help visualize what's going on.\n",
    "(fig, axes) = plt.subplots(nrows=2, ncols=2, figsize=(16,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "def show_predictions(ax, regressor, name):\n",
    "    predictions = y=regressor.predict(Xtest)\n",
    "    colouring = np.where(predictions < Ytest, 'red', 'green')\n",
    "    ax.scatter(x=Ytest, y=predictions, c=colouring)\n",
    "    ax.set_title(name + \" (Predictions vs Actual)\\n$%.2f loss\" % (lemonade_scorer(regressor, Xtest, Ytest),))\n",
    "    ax.plot([Ytest.min(),Ytest.max()],[Ytest.min(),Ytest.max()], linestyle='dotted', c='#e0e0ff')\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(name + \" predictions\")\n",
    "    ax.annotate(\"Predicted too many sales\\n(inexpensive mistake)\", xy=(Ytest.min()+1, Ytest.max()-2), color='green')\n",
    "    ax.annotate(\"Predicted too few sales\\n(expensive mistake)\", xy=(Ytest.max()-9,Ytest.min()+3), color='red')\n",
    "    \n",
    "show_predictions(axes[0][0], lemonade_ols, 'Least Squares')\n",
    "show_predictions(axes[0][1], lemonade_ts, 'Theil-Sen')\n",
    "show_predictions(axes[1][0], lemonade_ransac, 'RANSAC')\n",
    "show_predictions(axes[1][1], lemonade_huber, 'Huber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To put it plainly, what's happening here is that the least squares regression is trying hard to minimize the error from the five or so times that sales were far lower than would be expected based on temperature and humidity.\n",
    "\n",
    "But, saving a few cents on the input materials is overwhelmed by the losses incurred from not making high-enough predictions for typical days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "## Methods\n",
    "\n",
    "- If you want to generate a **quick approximation** for a good line through a data set, you can use Theil-Sen with or without a computer.\n",
    "\n",
    "- If you believe you have a **outliers in your data** (which are common) that don't affect the whole, then use one of the robust regression techniques: Huber, RANSAC, or Theil-Sen.\n",
    "\n",
    "\n",
    "## Scoring\n",
    "\n",
    "- **Ordinary least squares** will try to minimize large errors; while it depends on the scenario, this is usually not the right thing to minimize.\n",
    "\n",
    "- Minimizing the **median absolute error** is more effective.\n",
    "  \n",
    "- In most commercial scenarios, your goal is rarely accuracy. Make sure your scoring functions match the errors that your organization wants to minimize.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Ordinary least squares is related to Pearson's coefficient in the same way that Theil-Sen is related to Kendall's\n",
    "$\\tau$ coefficient; OLS is parametric, whereas Theil-Sen is non-parametric. There have been various attempts to remedy some of Theil-Sen's deficiencies. CRAN's paper on [Deming regression](https://cran.r-project.org/web/packages/deming/vignettes/deming.pdf]) discusses these topics.\n",
    "- [Linear Valuation Without OLS: The Theil-Sen Estimation Approach](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2276927) — financial and accounting data are easier to predict with Theil-Sen than OLS.\n",
    "- [Daniel Wedge wrote a *song* about RANSAC](http://danielwedge.com/ransac/) — he also has a song about $e$.\n",
    "- Watch this YouTube video on [robust regression with Huber weighting](https://www.youtube.com/watch?v=0drbiDPCuYQ).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
