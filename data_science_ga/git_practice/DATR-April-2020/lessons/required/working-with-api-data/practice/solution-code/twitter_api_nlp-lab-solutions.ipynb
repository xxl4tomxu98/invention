{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP Using the Twitter API: Guided Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/RNAEgP.jpg\" width=\"600\">\n",
    "\n",
    "### Can we correctly identify which of these two old men tweeted what?\n",
    "\n",
    "> *Note: this lab is intended to be a guided lab until the independent practice questions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "---\n",
    "\n",
    "We are going to attempt to classify whether a tweet comes from Trump or Sanders.  This lab involves multiple steps:\n",
    "- Create a developer account on Twitter\n",
    "- Create a method to pull a list of tweets from the Twitter API\n",
    "- Perform proper preprocessing on our text\n",
    "- Engineer sentiment feature in our dataset using TextBlob\n",
    "- Explore supervised classification techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Developer Registration\n",
    "---\n",
    "\n",
    "If you haven't registered a Twitter account yet, this is a requirement in order to have a \"developer\" account.\n",
    "\n",
    "[Twitter Rest API](https://dev.twitter.com/rest/public)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an \"App\"\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/HPBQbJ.jpg)\n",
    "\n",
    "Go to Twitter and register an \"app\" [apps.twitter.com](https://apps.twitter.com/).\n",
    "\n",
    "> **Note**: For the required website field you can put a placeholder.\n",
    "\n",
    "After you set up our app, you will only need to reference the cooresponding keys Twitter generates for our app.  These are the keys that we will use with our application to communicate with the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python Twitter API library\n",
    "\n",
    "---\n",
    "\n",
    "Someone was nice enough to build a Python libary for us. It makes pulling tweets simple: we only need to plug in our keys and start collecting data. The library we will be using is provided by [Python Twitter Tools](http://mike.verdone.ca/twitter/).\n",
    "\n",
    "To install it, just run the next frame (there is no conda package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitter in /anaconda3/lib/python3.7/site-packages (1.18.0)\n",
      "Requirement already satisfied: python-twitter in /anaconda3/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from python-twitter) (0.17.1)\n",
      "Requirement already satisfied: requests-oauthlib in /anaconda3/lib/python3.7/site-packages (from python-twitter) (1.2.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from python-twitter) (2.21.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda3/lib/python3.7/site-packages (from requests-oauthlib->python-twitter) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->python-twitter) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->python-twitter) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->python-twitter) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->python-twitter) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install twitter python-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Boring Twitter Rules\n",
    "---\n",
    "\n",
    "**Twitter notifies you they will rate limit your requests:**\n",
    "\n",
    ">When using application-only authentication, rate limits are determined globally for the entire application. If a method allows for 15 requests per rate limit window, then it allows you to make 15 requests per window — on behalf of your application. This limit is considered completely separately from per-user limits. https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Here's a quick overview of what Twitter says are \"the rules\":\n",
    "\n",
    "![](https://snag.gy/yJ6vIH.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About those Keys: OAuth Review\n",
    "---\n",
    "\n",
    "![](https://g.twimg.com/dev/documentation/image/appauth_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going on here?  Take a minute.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Application Keys\n",
    "---\n",
    "\n",
    "Take note of your application keys you will use to connect to Twitter and mine tweets from the official Bernie Sanders and Donald Trump twitter accounts:\n",
    "\n",
    "![](https://snag.gy/H1djQK.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TweetMiner` class structure\n",
    "\n",
    "---\n",
    "\n",
    "The following code will get you up and running, providing connectivity to twitter. The class has the ability to make requests and can eventually transform the JSON responses into DataFrames.\n",
    "\n",
    "This is a great example of using object-oriented Python to organize our code!\n",
    "\n",
    "> **Note:** \"request_limit\" is used in this class to limit the number of tweets that are pulled per instance request.  Setting it to something lower until you've worked the bugs out of your request, and captured the data you want, is essential to avoiding the rate limit blocks.\n",
    "\n",
    "### Twitter API key setup\n",
    "\n",
    "Fill the information below in with the keys for your account.\n",
    "\n",
    "- **consumer_key** - Find this in your app page under the \"Keys and Access Tokens\"\n",
    "- **consumer_secret** - Right under **consumer_key** in the \"Keys and Access Tokens\" tab\n",
    "- **access_token_key** - You will need to click the button to generate tokens to get this\n",
    "- **access_token_secret** - Also available after you generate tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter, re, datetime, pandas as pd\n",
    "\n",
    "twitter_keys = {\n",
    "    'consumer_key':        'lA6UQplM5sxIAIr83ueUl9sgE',\n",
    "    'consumer_secret':     'f94t4BD6Vj7aCVkX1qAIVwsP4x69J2vXvm61lTIuwb9GfmdsuP',\n",
    "    'access_token_key':    '1203210464-dYE1FvoUx1GjVcoyok3U1brWDpELBJEcSNGC1OC',\n",
    "    'access_token_secret': 'kuVli1j010RPdYTboz9iIyp8QxX6PqRApdi49baLqBgzo'\n",
    "}\n",
    "\n",
    "api = twitter.Api(\n",
    "    consumer_key         =   twitter_keys['consumer_key'],\n",
    "    consumer_secret      =   twitter_keys['consumer_secret'],\n",
    "    access_token_key     =   twitter_keys['access_token_key'],\n",
    "    access_token_secret  =   twitter_keys['access_token_secret']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetMiner(object):\n",
    "\n",
    "    result_limit    =   20    \n",
    "    api             =   False\n",
    "    data            =   []\n",
    "    \n",
    "    twitter_keys = {\n",
    "        'consumer_key':        'KmN03M1X1pImZ43sqdIu4yfnE',\n",
    "        'consumer_secret':     'ePlIrIX5VXbZnO7DBu1RbFlw5lOai9dQr9n5TZb6vxnIdrr5Fz',\n",
    "        'access_token_key':    '185036086-Q7K5IjuSoQZJwSIqD0wyHf6t62iPKatmfaPkriAM',\n",
    "        'access_token_secret': 'cYpQz3xWHQbLplOj8iSeiNSOmMcsTOXmWcKMrJ9buLj5d'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, keys_dict, api, result_limit = 20):\n",
    "        \n",
    "        self.api = api\n",
    "        self.twitter_keys = keys_dict\n",
    "        \n",
    "        self.result_limit = result_limit\n",
    "        \n",
    "\n",
    "    def mine_user_tweets(self, user=\"dyerrington\", mine_rewteets=False, max_pages=5):\n",
    "\n",
    "        data           =  []\n",
    "        last_tweet_id  =  False\n",
    "        page           =  1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            \n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit, max_id=last_tweet_id - 1)        \n",
    "            else:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit)\n",
    "                \n",
    "            for item in statuses:\n",
    "\n",
    "                mined = {\n",
    "                    'tweet_id':        item.id,\n",
    "                    'handle':          item.user.name,\n",
    "                    'retweet_count':   item.retweet_count,\n",
    "                    'text':            item.text,\n",
    "                    'mined_at':        datetime.datetime.now(),\n",
    "                    'created_at':      item.created_at,\n",
    "                }\n",
    "                \n",
    "                last_tweet_id = item.id\n",
    "                data.append(mined)\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the class\n",
    "---\n",
    "\n",
    "Make sure you pass the keys dictionary and the api as arguments.\n",
    "\n",
    "**Check:** call the object's `mine_user_tweets()` method, providing a user to pull the tweets of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = TweetMiner(twitter_keys, api, result_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanders = miner.mine_user_tweets(user=\"berniesanders\", max_pages=5)\n",
    "donald = miner.mine_user_tweets(user=\"realDonaldTrump\", max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 1116711991996030977, 'handle': 'Bernie Sanders', 'retweet_count': 1039, 'text': 'We did it! The grassroots movement has made 1 millions donations and counting to the Bernie Sanders presidential ca… https://t.co/xjzF7AFkGx', 'mined_at': datetime.datetime(2019, 4, 12, 11, 3, 15, 214546), 'created_at': 'Fri Apr 12 14:37:41 +0000 2019'}\n"
     ]
    }
   ],
   "source": [
    "print(sanders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 1116742282286440448, 'handle': 'Donald J. Trump', 'retweet_count': 8334, 'text': '....The Radical Left always seems to have an Open Borders, Open Arms policy – so this should make them very happy!', 'mined_at': datetime.datetime(2019, 4, 12, 11, 3, 15, 766990), 'created_at': 'Fri Apr 12 16:38:02 +0000 2019'}\n"
     ]
    }
   ],
   "source": [
    "print(donald[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tweet ouputs to a pandas DataFrame\n",
    "\n",
    "> *Hint: this is as easy as passing it to the DataFrame constructor!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Apr 12 14:37:41 +0000 2019</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2019-04-12 11:03:15.214546</td>\n",
       "      <td>1039</td>\n",
       "      <td>We did it! The grassroots movement has made 1 ...</td>\n",
       "      <td>1116711991996030977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Apr 12 04:00:00 +0000 2019</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2019-04-12 11:03:15.214562</td>\n",
       "      <td>763</td>\n",
       "      <td>It makes zero sense that we allow profit-drive...</td>\n",
       "      <td>1116551516997836802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Apr 12 00:00:00 +0000 2019</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2019-04-12 11:03:15.342283</td>\n",
       "      <td>1821</td>\n",
       "      <td>I don't often talk about myself, but I think i...</td>\n",
       "      <td>1116491116793212929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Apr 11 22:42:28 +0000 2019</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2019-04-12 11:03:15.342302</td>\n",
       "      <td>638</td>\n",
       "      <td>Medicare for All is an issue of civil rights. ...</td>\n",
       "      <td>1116471607084691457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Apr 11 22:42:00 +0000 2019</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2019-04-12 11:03:15.426987</td>\n",
       "      <td>335</td>\n",
       "      <td>RT @fshakir: What does a movement look like?\\n...</td>\n",
       "      <td>1116471490101358593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at          handle                   mined_at  \\\n",
       "0  Fri Apr 12 14:37:41 +0000 2019  Bernie Sanders 2019-04-12 11:03:15.214546   \n",
       "1  Fri Apr 12 04:00:00 +0000 2019  Bernie Sanders 2019-04-12 11:03:15.214562   \n",
       "2  Fri Apr 12 00:00:00 +0000 2019  Bernie Sanders 2019-04-12 11:03:15.342283   \n",
       "3  Thu Apr 11 22:42:28 +0000 2019  Bernie Sanders 2019-04-12 11:03:15.342302   \n",
       "4  Thu Apr 11 22:42:00 +0000 2019  Bernie Sanders 2019-04-12 11:03:15.426987   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0           1039  We did it! The grassroots movement has made 1 ...   \n",
       "1            763  It makes zero sense that we allow profit-drive...   \n",
       "2           1821  I don't often talk about myself, but I think i...   \n",
       "3            638  Medicare for All is an issue of civil rights. ...   \n",
       "4            335  RT @fshakir: What does a movement look like?\\n...   \n",
       "\n",
       "              tweet_id  \n",
       "0  1116711991996030977  \n",
       "1  1116551516997836802  \n",
       "2  1116491116793212929  \n",
       "3  1116471607084691457  \n",
       "4  1116471490101358593  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sanders).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Create the training data\n",
    "\n",
    "---\n",
    "\n",
    "Let's get our \"mined\" data from the Twitter API.  \n",
    "\n",
    "1. Mine Trump tweets\n",
    "- Create a tweet DataFrame\n",
    "- Mine Sanders tweets\n",
    "- Append the results to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to \"instantiate\" once.  Then we can call mine_user_tweets as much as we want.\n",
    "miner = TweetMiner(twitter_keys, api, result_limit=400)\n",
    "trump_tweets = miner.mine_user_tweets(\"realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "trump_df = pd.DataFrame(trump_tweets)\n",
    "print(trump_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_tweets = miner.mine_user_tweets('berniesanders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "bernie_df = pd.DataFrame(bernie_tweets) \n",
    "print(bernie_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.concat([trump_df, bernie_df], axis=0)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any interesting ngrams going on with Trump?\n",
    "---\n",
    "\n",
    "Set up a vectorizer from sklearn and fit the text of Trump's tweets with an ngram range from 2 to 4. Figure out what the most common ngrams are.\n",
    "\n",
    "> **Note:** It's up to you whether you want to remove stopwords or not. How does keeping or removing stopwords affect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 647),\n",
       " ('of the', 84),\n",
       " ('in the', 54),\n",
       " ('to the', 43),\n",
       " ('and the', 37),\n",
       " ('for the', 35),\n",
       " ('will be', 32),\n",
       " ('on the', 30),\n",
       " ('at the', 29),\n",
       " ('thank you', 29),\n",
       " ('the democrats', 28),\n",
       " ('our country', 26),\n",
       " ('that the', 25),\n",
       " ('fake news', 24),\n",
       " ('the border', 23),\n",
       " ('all of', 23),\n",
       " ('border security', 23),\n",
       " ('southern border', 22),\n",
       " ('president trump', 21),\n",
       " ('the united', 20)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2,4))\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "summaries = \"\".join(trump_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the ngrams for Bernie Sanders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 825),\n",
       " ('in the', 79),\n",
       " ('of the', 67),\n",
       " ('to the', 55),\n",
       " ('we must', 54),\n",
       " ('we have', 50),\n",
       " ('we are', 46),\n",
       " ('this country', 45),\n",
       " ('going to', 41),\n",
       " ('we can', 41),\n",
       " ('on the', 40),\n",
       " ('for the', 38),\n",
       " ('health care', 37),\n",
       " ('in this', 35),\n",
       " ('it is', 33),\n",
       " ('minimum wage', 33),\n",
       " ('for all', 32),\n",
       " ('we re', 32),\n",
       " ('we need', 30),\n",
       " ('united states', 29)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2,4))\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "summaries = \"\".join(bernie_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the tweets and building a model\n",
    "\n",
    "---\n",
    "\n",
    "To do classfication we will need to convert the tweets into a set of features.\n",
    "\n",
    "**You will need to:**\n",
    "- Vectorize input text data.\n",
    "- Intialize a model (try Logistic regression).\n",
    "- Train / Predict / cross-validate.\n",
    "- Evaluate the performance of the model.\n",
    "\n",
    "> **Bonus:** you may have noticed that there are website links in the tweets. What additional preprocessing steps can you do before building the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/fb/323c81288ab9b0b9cc955dcebfd04773d7982307c1a6d559b5a30000825b/textacy-0.6.3-py2.py3-none-any.whl (145kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 4.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx>=1.11 in /anaconda3/lib/python3.7/site-packages (from textacy) (2.2)\n",
      "Requirement already satisfied: requests>=2.10.0 in /anaconda3/lib/python3.7/site-packages (from textacy) (2.21.0)\n",
      "Collecting spacy>=2.0.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/f3/eb5aece989ecca541c4b5a439cb8a86fe5254b1521925b547609fb3aaf64/spacy-2.1.3.tar.gz (27.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 27.7MB 243kB/s ta 0:00:011  2% |▉                               | 737kB 5.8MB/s eta 0:00:05    5% |█▉                              | 1.5MB 8.9MB/s eta 0:00:03    43% |█████████████▉                  | 11.9MB 24.4MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting srsly>=0.0.5 (from textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/b2/d2cc9f5aa5a458aca45d8689279b41d4b42ba4e8c63b0cb6a9f009340fd0/srsly-0.0.5-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting python-levenshtein>=0.12.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ijson>=2.3 (from textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e9/8508c5f4987ba238a2b169e582c1f70a47272b22a2f1fb06b9318201bb9e/ijson-2.3-py2.py3-none-any.whl\n",
      "Collecting pyphen>=0.9.4 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /anaconda3/lib/python3.7/site-packages (from textacy) (1.16.2)\n",
      "Collecting pyemd>=0.3.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/c5/7fea8e7a71cd026b30ed3c40e4c5ea13a173e28f8855da17e25271e8f545/pyemd-0.5.1.tar.gz (91kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 9.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cytoolz>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from textacy) (0.9.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from textacy) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from textacy) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /anaconda3/lib/python3.7/site-packages (from textacy) (4.31.1)\n",
      "Collecting ftfy<5.0.0,>=4.2.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mwparserfromhell>=0.4.4 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a2/5d537c16e761db25592b2e69c0071aee475f200ef231ee5ca1a499ae54bc/mwparserfromhell-0.5.3.tar.gz (132kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=2.0.0 (from textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/39/2b/d87fc2369242bd743883232c463f28205902b8579cb68dcf5b11eee1652f/cachetools-3.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.3.0 in /anaconda3/lib/python3.7/site-packages (from networkx>=1.11->textacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy) (2019.3.9)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/bd/faace403086ee922afc74e5615cb8c21020fcf5d5667314e943c08f71fde/murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/fe/2f2e8c91541785f2abe0d51f37eb00356513b9ff3d24fb27fd5b59e18264/preshed-2.0.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/11/37da628920bf2999bd8c4ffc40908413622486d5dbc4e60d87a58c428367/cymem-2.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy>=2.0.0->textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/b1/d7df83813ee3c42d46e6ddf4d4f1d9bd35a4735827d35b7f02539bea3136/thinc-7.0.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 1.9MB/s eta 0:00:01    31% |██████████                      | 880kB 6.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.3.0,>=0.2.2 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/41/9e934e2b8a2cdae447ed1923a94f98c2d70c898b65af6635f5fe55f7ed4d/blis-0.2.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting jsonschema<3.0.0,>=2.6.0 (from spacy>=2.0.0->textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/de/47e35a97b2b05c2fadbec67d44cfcdcd09b8086951b331d82de90d2912da/jsonschema-2.6.0-py2.py3-none-any.whl\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy>=2.0.0->textacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/6c/0376977df1ba9f0ec27835d80456d9284c79737cb5205649451db1181f01/wasabi-0.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from python-levenshtein>=0.12.0->textacy) (40.8.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: html5lib in /anaconda3/lib/python3.7/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9 in /anaconda3/lib/python3.7/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (1.12.0)\n",
      "Requirement already satisfied: webencodings in /anaconda3/lib/python3.7/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (0.5.1)\n",
      "Building wheels for collected packages: spacy\n",
      "  Building wheel for spacy (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/abc/Library/Caches/pip/wheels/67/16/2b/95f509d3ed9752e575649e4072f072363e75146ac06d09195a\n",
      "Successfully built spacy\n",
      "Building wheels for collected packages: python-levenshtein, pyemd, ftfy, mwparserfromhell\n",
      "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/abc/Library/Caches/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "  Building wheel for pyemd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/abc/Library/Caches/pip/wheels/e4/ba/b0/1f4178a35c916b22fc51dc56f278125d4b8cfb0592e5f0cc24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/abc/Library/Caches/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "  Building wheel for mwparserfromhell (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/abc/Library/Caches/pip/wheels/68/e9/75/f7fcd23cc94c77fc6d07793abcabda7c8829042c177f47028b\n",
      "Successfully built python-levenshtein pyemd ftfy mwparserfromhell\n",
      "Installing collected packages: plac, murmurhash, srsly, cymem, preshed, blis, wasabi, thinc, jsonschema, spacy, python-levenshtein, ijson, pyphen, pyemd, ftfy, mwparserfromhell, cachetools, textacy\n",
      "  Found existing installation: jsonschema 3.0.1\n",
      "    Uninstalling jsonschema-3.0.1:\n",
      "      Successfully uninstalled jsonschema-3.0.1\n",
      "Successfully installed blis-0.2.4 cachetools-3.1.0 cymem-2.0.2 ftfy-4.4.3 ijson-2.3 jsonschema-2.6.0 murmurhash-1.0.2 mwparserfromhell-0.5.3 plac-0.9.6 preshed-2.0.1 pyemd-0.5.1 pyphen-0.9.5 python-levenshtein-0.12.0 spacy-2.1.3 srsly-0.0.5 textacy-0.6.3 thinc-7.0.4 wasabi-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess_text() got an unexpected keyword argument 'transliterate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5480baae3065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mno_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_emails\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_phone_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_currency_symbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               no_punct=True, no_accents=True)\n\u001b[0;32m---> 11\u001b[0;31m               for x in tweet_text]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-5480baae3065>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mno_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_emails\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_phone_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_currency_symbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               no_punct=True, no_accents=True)\n\u001b[0;32m---> 11\u001b[0;31m               for x in tweet_text]\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: preprocess_text() got an unexpected keyword argument 'transliterate'"
     ]
    }
   ],
   "source": [
    "# BONUS\n",
    "# Using the textacy package to do some more comprehensive preprocessing\n",
    "# http://textacy.readthedocs.io/en/latest/\n",
    "import numpy as np\n",
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "tweet_text = tweets['text'].values\n",
    "clean_text = [preprocess_text(x, fix_unicode=True, lowercase=True, transliterate=False,\n",
    "                              no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True,\n",
    "                              no_punct=True, no_accents=True)\n",
    "              for x in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Report: “ANTI-TRUMP FBI AGENT LED CLINTON EMAIL PROBE”  Now it all starts to make sense!'\n",
      " 'People who lost money when the Stock Market went down 350 points based on the False and Dishonest reporting of Bria… https://t.co/4MT8wja1pn'\n",
      " 'After years of Comey, with the phony and dishonest Clinton investigation (and more), running the FBI, its reputatio… https://t.co/gdbdgXZ57e']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report antitrump fbi agent led clinton email probe now it all starts to make sense', 'people who lost money when the stock market went down 350 points based on the false and dishonest reporting of bria url', 'after years of comey with the phony and dishonest clinton investigation and more running the fbi its reputatio url']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498746867168\n"
     ]
    }
   ],
   "source": [
    "# target is the handle.\n",
    "# make trump 1 and sanders 0\n",
    "y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values\n",
    "\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1995, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Preprocess our text data to Tfidf\n",
    "tfv = TfidfVectorizer(ngram_range=(1,4), max_features=2000)\n",
    "X = tfv.fit_transform(clean_text).todense()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85        0.905       0.88        0.89        0.89        0.88944724\n",
      "  0.90954774  0.88944724  0.87939698  0.86432161]\n",
      "0.884716080402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validate the accuracy:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(LogisticRegression(), X, y, cv=10)\n",
    "\n",
    "print(accuracies)\n",
    "print(np.mean(accuracies))\n",
    "\n",
    "# Setup logistic regression (or try another classification method here)\n",
    "estimator = LogisticRegression()\n",
    "estimator.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Very good accuracy considering the baseline is 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the predicted probability for a random Sanders and Trump tweet\n",
    "---\n",
    "\n",
    "Below are provided a couple of tweets from both Sanders and Trump. I'm sure you can figure out on your own which one is which.\n",
    "\n",
    "Estimate the predicted probability of being trump for the two tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77807224,  0.22192776],\n",
       "       [ 0.32989667,  0.67010333]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep our source as TfIdf vectors\n",
    "source_test = [\n",
    "    \"Demanding that the wealthy and the powerful start paying their fair share of taxes that's exactly what the American people want.\",\n",
    "    \"Crooked Hillary is spending tremendous amounts of Wall Street money on false ads against me. She is a very dishonest person!\"\n",
    "]\n",
    "\n",
    "############\n",
    "# NOTE:  Do not re-initialize the tfidf vectorizor or the feature space willbe overwritten and\n",
    "# hence your transform will not match the number of features you trained your model on.\n",
    "#\n",
    "# This is why you only need to \"transform\" since you already \"fit\" previously\n",
    "#\n",
    "####\n",
    "\n",
    "Xtest = tfv.transform(source_test)\n",
    "\n",
    "# Predict using previously trained logist regression `estimator`\n",
    "estimator.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The 1st column is probability of being Bernie, and 2nd Trump. The classifier is getting it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent practice questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pull tweets for some new users.\n",
    "\n",
    "Experiment with using more data.  The API will not like it if you blow through their limits - be careful.  Try to grab only what you need one time, then work on the copy of the objects that are returned.  \n",
    "\n",
    "> Read the documentation about rate limits and see if you can get enough without hitting the rate limit.  Are there any options available in the API to avoid such a problem?\n",
    "\n",
    "**Pull tweets for more than two different users of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining tweets for: dril\n",
      "Mining tweets for: LaziestCanine\n",
      "Mining tweets for: ch000ch\n"
     ]
    }
   ],
   "source": [
    "# We deviate from trump / sanders using student tweets here to illustrate the NLP pipeine with twitter data\n",
    "\n",
    "twitter_handles = [\"dril\", \"LaziestCanine\",'ch000ch']\n",
    "tweets = {}\n",
    "\n",
    "for twitter_handle in twitter_handles:\n",
    "    print(\"Mining tweets for: {}\".format(twitter_handle))\n",
    "    miner = TweetMiner(twitter_keys, api, result_limit=500)\n",
    "    tweets[twitter_handle] = miner.mine_user_tweets(user=twitter_handle, max_pages=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5992, 6)\n"
     ]
    }
   ],
   "source": [
    "multi = pd.DataFrame(tweets['dril'])\n",
    "multi = multi.append(pd.DataFrame(tweets['LaziestCanine']))\n",
    "multi = multi.append(pd.DataFrame(tweets['ch000ch']))\n",
    "\n",
    "print(multi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wint        1998\n",
       "Lazy dog    1998\n",
       "chuuch      1996\n",
       "Name: handle, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi.handle.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a multi-class classification model to distinguish between the users.\n",
    "\n",
    "Try a new type of model than we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_text = multi['text'].values\n",
    "clean_text = [preprocess_text(x, fix_unicode=True, lowercase=True, transliterate=False,\n",
    "                              no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True,\n",
    "                              no_punct=True, no_accents=True)\n",
    "              for x in tweet_text]\n",
    "\n",
    "y = multi['handle'].map(lambda x: 0 if x == 'wint' else 1 if x == 'Lazy dog' else 2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,3), max_features=2500)\n",
    "X = tfv.fit_transform(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=250, verbose=1)\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 0.7018909899888766\n",
      "KNN: 0.40378197997775306\n"
     ]
    }
   ],
   "source": [
    "# Random forest score:\n",
    "print('RF: {}'.format(rf.score(X_test, y_test)))\n",
    "print('KNN: {}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wint        0.333445\n",
       "Lazy dog    0.333445\n",
       "chuuch      0.333111\n",
       "Name: handle, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline score:\n",
    "multi.handle.value_counts() / multi.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_yhat = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.22      0.31       636\n",
      "          1       0.38      0.73      0.50       584\n",
      "          2       0.40      0.28      0.33       578\n",
      "\n",
      "avg / total       0.44      0.40      0.38      1798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, rf_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140 346 150]\n",
      " [ 66 425  93]\n",
      " [ 61 356 161]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, rf_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the most and least \"distinctive\" tweets for each user?\n",
    "\n",
    "To find this, identify the tweet that has the highest (correct) predicted probability of being that user's tweet for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X, y)\n",
    "\n",
    "pp = rf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.928,  0.02 ,  0.052],\n",
       "       [ 0.848,  0.008,  0.144],\n",
       "       [ 0.836,  0.092,  0.072],\n",
       "       [ 0.724,  0.084,  0.192],\n",
       "       [ 0.876,  0.012,  0.112]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pd.DataFrame(pp, columns=['dril_pp', 'laziestcanine_pp', 'ch000ch_pp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5992, 6)\n",
      "(5992, 3)\n"
     ]
    }
   ],
   "source": [
    "print(multi.shape)\n",
    "print(pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>index</th>\n",
       "      <th>dril_pp</th>\n",
       "      <th>laziestcanine_pp</th>\n",
       "      <th>ch000ch_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Dec 03 23:12:50 +0000 2017</td>\n",
       "      <td>wint</td>\n",
       "      <td>2017-12-04 01:01:34.631592</td>\n",
       "      <td>5399</td>\n",
       "      <td>getting pissed off at the idea of someone goin...</td>\n",
       "      <td>937459644229828608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Dec 03 15:35:19 +0000 2017</td>\n",
       "      <td>wint</td>\n",
       "      <td>2017-12-04 01:01:34.631604</td>\n",
       "      <td>1225</td>\n",
       "      <td>#WorldwideHandsomeDay looking dumb as a dog in...</td>\n",
       "      <td>937344504872558593</td>\n",
       "      <td>1</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      created_at handle                   mined_at  \\\n",
       "0      0  Sun Dec 03 23:12:50 +0000 2017   wint 2017-12-04 01:01:34.631592   \n",
       "1      1  Sun Dec 03 15:35:19 +0000 2017   wint 2017-12-04 01:01:34.631604   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0           5399  getting pissed off at the idea of someone goin...   \n",
       "1           1225  #WorldwideHandsomeDay looking dumb as a dog in...   \n",
       "\n",
       "             tweet_id  index  dril_pp  laziestcanine_pp  ch000ch_pp  \n",
       "0  937459644229828608      0    0.928             0.020       0.052  \n",
       "1  937344504872558593      1    0.848             0.008       0.144  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1)\n",
    "tweets_pp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most dril: @HarshilShah1910 shut the fuck up\n",
      "Least dril: @antonwheel thanks\n"
     ]
    }
   ],
   "source": [
    "print('Most dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=False).text.values[0]))\n",
    "print('Least dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=True).text.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most LaziestCanine: my body is my journal and my tattoos are my story https://t.co/k4F1rUFzn5\n",
      "Least LaziestCanine: @Jaclyn_Mariee_ lmao tru\n"
     ]
    }
   ],
   "source": [
    "print('Most LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=False).text.values[0]))\n",
    "print('Least LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=True).text.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most chuuch: @me_db lol\n",
      "Least chuuch: https://t.co/WwvtlPWQMC\n"
     ]
    }
   ],
   "source": [
    "print('Most chuuch: {}'.format(tweets_pp[tweets_pp.handle == 'chuuch'].sort_values('ch000ch_pp', ascending=False).text.values[0]))\n",
    "print('Least chuuch: {}'.format(tweets_pp[tweets_pp.handle == 'chuuch'].sort_values('ch000ch_pp', ascending=True).text.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
