{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Guided Practice with Topic Modeling and LDA\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> **Note: this lab is intended to be a guided lab with the instructor.**\n",
    "\n",
    "In practice it would be a very rare to need to build an unsupervised topic model like LDA from scratch. Lucky for us, sklearn comes with LDA(Latent Dirichlet Allocation) topic modeling functionality. Another popular LDA module which we will explore in this lab is from the `gensim` package. \n",
    "\n",
    "Let's explore a brief walkthrough of LDA and topic modeling using gensim. We will work with a small collection of documents represented as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the packages and create the small \"documents\".\n",
    "\n",
    "You may need to install the gensim package with `pip` or `conda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "df        = pd.DataFrame(documents, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brocolli is good to eat. My brother likes to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mother spends a lot of time driving my brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some health experts suggest that driving may c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I often feel pressure to perform well at schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health professionals say that brocolli is good...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Brocolli is good to eat. My brother likes to e...\n",
       "1  My mother spends a lot of time driving my brot...\n",
       "2  Some health experts suggest that driving may c...\n",
       "3  I often feel pressure to perform well at schoo...\n",
       "4  Health professionals say that brocolli is good..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load stop words either from NLTK or sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13305\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "len(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use CountVectorizer to transform our text, taking out the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brocolli': 4, 'good': 12, 'eat': 9, 'brother': 5, 'likes': 15, 'mother': 18, 'spends': 28, 'lot': 16, 'time': 31, 'driving': 8, 'around': 0, 'baseball': 1, 'practice': 22, 'health': 13, 'experts': 10, 'suggest': 29, 'may': 17, 'cause': 6, 'increased': 14, 'tension': 30, 'blood': 3, 'pressure': 23, 'often': 20, 'feel': 11, 'perform': 21, 'well': 32, 'school': 26, 'never': 19, 'seems': 27, 'drive': 7, 'better': 2, 'professionals': 24, 'say': 25}\n",
      "{'brocolli': 3, 'good': 11, 'eat': 8, 'brother': 4, 'likes': 14, 'mother': 16, 'spends': 23, 'lot': 15, 'time': 26, 'driving': 7, 'baseball': 0, 'practice': 18, 'health': 12, 'experts': 9, 'suggest': 24, 'cause': 5, 'increased': 13, 'tension': 25, 'blood': 2, 'pressure': 19, 'feel': 10, 'perform': 17, 'school': 22, 'drive': 6, 'better': 1, 'professionals': 20, 'say': 21}\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "vect1 = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "df_dtm1 = vect1.fit_transform(X)\n",
    "df_dtm2 = vect2.fit_transform(X)\n",
    "# sklearn stopwords has more items than that of the NLTK, therefore eliminates more stopwords \n",
    "print(vect1.vocabulary_)\n",
    "print(vect2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract the tokens that remain after stopword removal.\n",
    "\n",
    "The `.vocabulary_` attribute of the vectorizer contains a dictionary of terms. There is also the built-in function `.get_feature_names()` which will extract the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['around', 'baseball', 'better', 'blood', 'brocolli', 'brother', 'cause', 'drive', 'driving', 'eat', 'experts', 'feel', 'good', 'health', 'increased', 'likes', 'lot', 'may', 'mother', 'never', 'often', 'perform', 'practice', 'pressure', 'professionals', 'say', 'school', 'seems', 'spends', 'suggest', 'tension', 'time', 'well']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "print(vect1.get_feature_names())\n",
    "len(vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball', 'better', 'blood', 'brocolli', 'brother', 'cause', 'drive', 'driving', 'eat', 'experts', 'feel', 'good', 'health', 'increased', 'likes', 'lot', 'mother', 'perform', 'practice', 'pressure', 'professionals', 'say', 'school', 'spends', 'suggest', 'tension', 'time']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vect2.get_feature_names())\n",
    "len(vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get counts of tokens.\n",
    "\n",
    "Convert the matrix from the vectorizer to a dense matrix, then sum by column to get the counts per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>around</th>\n",
       "      <th>baseball</th>\n",
       "      <th>better</th>\n",
       "      <th>blood</th>\n",
       "      <th>brocolli</th>\n",
       "      <th>brother</th>\n",
       "      <th>cause</th>\n",
       "      <th>drive</th>\n",
       "      <th>driving</th>\n",
       "      <th>eat</th>\n",
       "      <th>...</th>\n",
       "      <th>pressure</th>\n",
       "      <th>professionals</th>\n",
       "      <th>say</th>\n",
       "      <th>school</th>\n",
       "      <th>seems</th>\n",
       "      <th>spends</th>\n",
       "      <th>suggest</th>\n",
       "      <th>tension</th>\n",
       "      <th>time</th>\n",
       "      <th>well</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   around  baseball  better  blood  brocolli  brother  cause  drive  driving  \\\n",
       "0       1         1       1      1         3        3      1      1        2   \n",
       "\n",
       "   eat  ...  pressure  professionals  say  school  seems  spends  suggest  \\\n",
       "0    2  ...         2              1    1       1      1       1        1   \n",
       "\n",
       "   tension  time  well  \n",
       "0        1     1     1  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "df_dtm1_arr = df_dtm1.toarray()\n",
    "df1 = df_dtm1_arr.sum(axis=0)\n",
    "df1=pd.DataFrame(df1.reshape(1,len(vect1.get_feature_names())), columns=vect1.get_feature_names())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>better</th>\n",
       "      <th>blood</th>\n",
       "      <th>brocolli</th>\n",
       "      <th>brother</th>\n",
       "      <th>cause</th>\n",
       "      <th>drive</th>\n",
       "      <th>driving</th>\n",
       "      <th>eat</th>\n",
       "      <th>experts</th>\n",
       "      <th>...</th>\n",
       "      <th>perform</th>\n",
       "      <th>practice</th>\n",
       "      <th>pressure</th>\n",
       "      <th>professionals</th>\n",
       "      <th>say</th>\n",
       "      <th>school</th>\n",
       "      <th>spends</th>\n",
       "      <th>suggest</th>\n",
       "      <th>tension</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseball  better  blood  brocolli  brother  cause  drive  driving  eat  \\\n",
       "0         1       1      1         3        3      1      1        2    2   \n",
       "\n",
       "   experts  ...  perform  practice  pressure  professionals  say  school  \\\n",
       "0        1  ...        1         1         2              1    1       1   \n",
       "\n",
       "   spends  suggest  tension  time  \n",
       "0       1        1        1     1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dtm2_arr = df_dtm2.toarray()\n",
    "df2 = df_dtm2_arr.sum(axis=0)\n",
    "df2=pd.DataFrame(df2.reshape(1,len(vect2.get_feature_names())), columns=vect2.get_feature_names())\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Setup the vocabulary dictionary\n",
    "\n",
    "First we need to setup the vocabulary.  Gensim's LDA expects our vocabulary to be in a format where the dictionary keys are the column indices and the values are the words themselves.\n",
    "\n",
    "Create this dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'baseball',\n",
       " 1: 'better',\n",
       " 2: 'blood',\n",
       " 3: 'brocolli',\n",
       " 4: 'brother',\n",
       " 5: 'cause',\n",
       " 6: 'drive',\n",
       " 7: 'driving',\n",
       " 8: 'eat',\n",
       " 9: 'experts',\n",
       " 10: 'feel',\n",
       " 11: 'good',\n",
       " 12: 'health',\n",
       " 13: 'increased',\n",
       " 14: 'likes',\n",
       " 15: 'lot',\n",
       " 16: 'mother',\n",
       " 17: 'perform',\n",
       " 18: 'practice',\n",
       " 19: 'pressure',\n",
       " 20: 'professionals',\n",
       " 21: 'say',\n",
       " 22: 'school',\n",
       " 23: 'spends',\n",
       " 24: 'suggest',\n",
       " 25: 'tension',\n",
       " 26: 'time'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "vocab1 = {}\n",
    "for i in range(len(vect1.get_feature_names())):\n",
    "    vocab1[i] = vect1.get_feature_names()[i]\n",
    "vocab2 = {}\n",
    "for i in range(len(vect2.get_feature_names())):\n",
    "    vocab2[i] = vect2.get_feature_names()[i]\n",
    "vocab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create a token to id mapping with gensim's `corpora.Dictionary`\n",
    "\n",
    "This dictionary class is a more standard way to work with with gensim models. There are a few standard steps we should go through:\n",
    "\n",
    "**7.1. Count the frequency of words.**\n",
    "\n",
    "We can do this easily with the python `defaultdict(int)`, which doesn't require us to already have the key in the dictionary to be able to add to it:\n",
    "\n",
    "```python\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Brocolli': 1,\n",
       "             'is': 2,\n",
       "             'good': 3,\n",
       "             'to': 6,\n",
       "             'eat.': 1,\n",
       "             'My': 2,\n",
       "             'brother': 3,\n",
       "             'likes': 1,\n",
       "             'eat': 1,\n",
       "             'brocolli,': 1,\n",
       "             'but': 2,\n",
       "             'not': 1,\n",
       "             'my': 4,\n",
       "             'mother.': 1,\n",
       "             'mother': 2,\n",
       "             'spends': 1,\n",
       "             'a': 1,\n",
       "             'lot': 1,\n",
       "             'of': 1,\n",
       "             'time': 1,\n",
       "             'driving': 2,\n",
       "             'around': 1,\n",
       "             'baseball': 1,\n",
       "             'practice.': 1,\n",
       "             'Some': 1,\n",
       "             'health': 1,\n",
       "             'experts': 1,\n",
       "             'suggest': 1,\n",
       "             'that': 2,\n",
       "             'may': 1,\n",
       "             'cause': 1,\n",
       "             'increased': 1,\n",
       "             'tension': 1,\n",
       "             'and': 1,\n",
       "             'blood': 1,\n",
       "             'pressure.': 1,\n",
       "             'I': 1,\n",
       "             'often': 1,\n",
       "             'feel': 1,\n",
       "             'pressure': 1,\n",
       "             'perform': 1,\n",
       "             'well': 1,\n",
       "             'at': 1,\n",
       "             'school,': 1,\n",
       "             'never': 1,\n",
       "             'seems': 1,\n",
       "             'drive': 1,\n",
       "             'do': 1,\n",
       "             'better.': 1,\n",
       "             'Health': 1,\n",
       "             'professionals': 1,\n",
       "             'say': 1,\n",
       "             'brocolli': 1,\n",
       "             'for': 1,\n",
       "             'your': 1,\n",
       "             'health.': 1})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A: remember that documents is a list\n",
    "frequency = defaultdict(int)\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2 Remove any words that only appear once, or appear in the stopwords.**\n",
    "\n",
    "Iterate through the documents and only keep useful words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'good': 3, 'My': 2, 'brother': 3, 'mother': 2, 'driving': 2})\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "# list of delete keys \n",
    "key_del = [] \n",
    "for key, val in frequency.items(): \n",
    "    if val == 1 or key in stopwords.words('english'): \n",
    "        key_del.append(key) \n",
    "          \n",
    "for i in key_del: \n",
    "    del frequency[i] \n",
    "      \n",
    "# Modified Dictionary      \n",
    "print(frequency) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3 Create the `corpora.Dictionary` object with the retained tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My': 0, 'brother': 1, 'driving': 2, 'good': 3, 'mother': 4}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A: input has to be a list format\n",
    "dict = corpora.Dictionary([frequency.keys()])\n",
    "dict.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 Use the `dictionary.doc2bow()` function to convert the texts to bag-of-word representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A: corpus is the BOW representation\n",
    "corpus = [dictionary.doc2bow(doc) for doc in [frequency.keys()]]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why should we use this process?**\n",
    "\n",
    "The main advantage is that this dictionary object has quick helper functions.\n",
    "\n",
    "However, there are also some major performance advantages if you ever want to save your model to a file, then load it at a later time.  Tokenizations can take a while to be computed, especially when your text files are quite large. You can save these post-computed dictionary items to file, then load them from disk later which is quite a bit faster.  Also, it's possible to add new documents to your corpus without having to re-tokenize your entire set.  This is great for online systems that can take new documents on demmand.  \n",
    "\n",
    "As you work with larger datasets with text, this is a much better way to handle LDA and other Gensim models from a performance point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Set up the LDA model\n",
    "\n",
    "We can create the gensim LDA model object like so:\n",
    "\n",
    "```python\n",
    "lda = models.LdaModel(\n",
    "    # supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    # or alternatively use the corpus object created with the dictionary in the previous frame!\n",
    "    # corpus,\n",
    "    # The number of topics we want:\n",
    "    num_topics  =  3,\n",
    "    # how many passes over the vocabulary:\n",
    "    passes      =  20,\n",
    "    # The id2word vocabulary we made ourselves\n",
    "    id2word     =  vocab\n",
    "    # or use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "lda = models.LdaModel(\n",
    "    # supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object\n",
    "    # matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    # or alternatively use the corpus object created with the dictionary in the previous frame!\n",
    "    corpus,\n",
    "    # The number of topics we want:\n",
    "    num_topics  =  3,\n",
    "    # how many passes over the vocabulary:\n",
    "    passes      =  20,\n",
    "    # The id2word vocabulary we made ourselves\n",
    "    id2word     =  vocab1,\n",
    "    # or use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    "    random_state = 123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Look at the topics\n",
    "\n",
    "The model has a `.print_topics` function that accepts the number of topics to print and number of words per topic. The number before the word is the probability of occurance for that word in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.030*\"baseball\" + 0.030*\"blood\" + 0.030*\"better\" + 0.030*\"brocolli\" + 0.030*\"around\" + 0.030*\"professionals\" + 0.030*\"spends\" + 0.030*\"perform\" + 0.030*\"practice\" + 0.030*\"pressure\"'), (1, '0.030*\"brocolli\" + 0.030*\"better\" + 0.030*\"around\" + 0.030*\"blood\" + 0.030*\"baseball\" + 0.030*\"professionals\" + 0.030*\"spends\" + 0.030*\"perform\" + 0.030*\"practice\" + 0.030*\"pressure\"'), (2, '0.083*\"baseball\" + 0.083*\"around\" + 0.083*\"better\" + 0.083*\"blood\" + 0.083*\"brocolli\" + 0.021*\"professionals\" + 0.021*\"spends\" + 0.021*\"perform\" + 0.021*\"practice\" + 0.021*\"pressure\"')]\n",
      "[(0, '0.030*\"baseball\" + 0.030*\"blood\" + 0.030*\"better\" + 0.030*\"brocolli\" + 0.030*\"around\" + 0.030*\"professionals\" + 0.030*\"spends\" + 0.030*\"perform\" + 0.030*\"practice\" + 0.030*\"pressure\"'), (1, '0.030*\"brocolli\" + 0.030*\"better\" + 0.030*\"around\" + 0.030*\"blood\" + 0.030*\"baseball\" + 0.030*\"professionals\" + 0.030*\"spends\" + 0.030*\"perform\" + 0.030*\"practice\" + 0.030*\"pressure\"'), (2, '0.083*\"baseball\" + 0.083*\"around\" + 0.083*\"better\" + 0.083*\"blood\" + 0.083*\"brocolli\" + 0.021*\"professionals\" + 0.021*\"spends\" + 0.021*\"perform\" + 0.021*\"practice\" + 0.021*\"pressure\"')]\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "print(lda.print_topics())\n",
    "print(lda.show_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Get the topic scores for a document\n",
    "\n",
    "The `.get_document_topics` function accepts a bag-of-words representation for a document and returns the scores for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.05613149), (1, 0.88773704), (2, 0.056131504)]\n",
      "[(0, 0.056131545), (1, 0.887737), (2, 0.056131504)]\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "print(lda.get_document_topics(corpus[0]))\n",
    "print(lda.get_document_topics(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Label and visualize the topics\n",
    "\n",
    "Lets come up with some high level labels. This is the subjective part of LDA. What do the word probabilties that represent topics mean?  Let's make some up.\n",
    "\n",
    "Plot a heatmap of the topic probabilities for each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fit an LDA model with sklearn\n",
    "\n",
    "Sklearn's LDA model is in the decomposition submodule:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "```\n",
    "\n",
    "One of the greatest benefits of the sklearn implementation is that it comes with the familiar `.fit()`, `.transform()` and `.fit_transform()` methods.\n",
    "\n",
    "**12.1 Initialize and fit an sklearn LDA with `n_topics=3` on our output from the CountVectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.2 Print out the topic-word distributions using the `.components_` attribute.**\n",
    "\n",
    "Each row of this matrix represents a topic, and the columns are the words. (These are not probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 Use the `.transform()` method to convert the matrix into the topic scores.**\n",
    "\n",
    "These are the document-topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Further steps\n",
    "\n",
    "This has been a very basic example.  LDA typically doesn't perform well on very small datasets.  You should try to see how it behaves on your own using a larger text dataset.  Keep in mind: finding the optimal number of topics can be tricky and subjective.\n",
    "\n",
    "**Generally, you should consider:**\n",
    "- How well topics are applied to documents overall\n",
    "- The strength of topics overall, to all documents\n",
    "- Improving preprocessing such as stopword removal\n",
    "- Building a nice web interface to explore your documents (see: [LDAExplorer](https://github.com/dyerrington/LDAExplorer), and [pyLDAvis](https://github.com/bmabey/pyLDAvis/blob/master/README.rst))\n",
    "\n",
    "These general guidelines should help you tune your hyperparameter **K** for number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
